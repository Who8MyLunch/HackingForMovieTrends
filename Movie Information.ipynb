{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from __future__ import division, print_function, unicode_literals\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import re\n",
      "\n",
      "import IPython.display\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import data_io\n",
      "import utilities\n",
      "\n",
      "import BeautifulSoup\n",
      "import arrow\n",
      "import twython\n",
      "import textblob\n",
      "import requests\n",
      "\n",
      "# Keyboard shortcuts: http://ipython.org/ipython-doc/stable/interactive/notebook.html#keyboard-shortcuts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Task Objective\n",
      "\n",
      "My objective for this effort is to demonstrate my data processing and analysis capabilities outside of my tradiational hyperspectral remote sensing work.  I have played in that arena for a long time and picked up a good number of modeling and analysis skills.  This present effort is meant to be a quick example of processing unfamiliar data using new tools and protocols.  This work needs to be quick, efficient, and have a clear punch line.  This notebook is where plan to explore these tools and the data they help me fetch.  I'll use another notebook for the analysis once all the data is sorted out.\n",
      "\n",
      "I made a statement a few days ago indicating that I would like to solve new types of problems.  For example, I might treat a new movie as a collection of word feature vectors pulled out of a Twitter feed.  I would then make statistical associations with other movies having known performance characteristics such as viewer retention and engagement.  The validity of the association process could be verified by testing with lablled data.  Results from such a process might be useful for someone's planning efforts.\n",
      "\n",
      "# Early Morning Thoughts\n",
      "\n",
      "## TextBlob\n",
      "\n",
      "Very very early this morning I could not sleep as I kept thinking about this little task.  I kept reviewing in my head details of the objective in the section above.  I need a way to make sense of text describing a movie.  A quick search on Github turned up this great Python package:\n",
      "[TextBlob](https://github.com/sloria/TextBlob).  The text from the website says:\n",
      "\n",
      "    A library for processing textual data. It provides a simple API for diving into common natural\n",
      "    language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction,\n",
      "    sentiment analysis, classification, translation, and more.\n",
      "\n",
      "Behind the scenes it uses the packages [NLTK](http://www.nltk.org/) and\n",
      "[patterns](http://www.clips.ua.ac.be/pages/pattern-en).  I haven't done much at all with natural\n",
      "language text processing, but this tool looks like a great place to start!  TextBlob will return two metrics describing the sentiment of a chunk of text: **Polarity** and **Subjectivity**.  Those two numbers will be a great starting point for visualizing this stuff.\n",
      "\n",
      "## Open Movie Database API: IMDb and RottenTomatoes\n",
      "\n",
      "Next, I found a nice web API for querrying information from both IMDb and RottenTomatoes:\n",
      "[The OMDb API](http://www.omdbapi.com/).  This site used to reside at this other address http://imdbapi.com/, but not anymore.  Take a look over there for an interesting writeup of the site owner's interaction with IMDb.com's lawyers.  Very clever!  Anyhow, you can use this service to very easily search for movie information pulled from IMDb and RottenTomatoes.\n",
      "\n",
      "## Twitter API\n",
      "\n",
      "I found several Python packages on Github searching for Twitter's API service, here are two that seem most well-maintained: https://github.com/geduldig/TwitterAPI and https://github.com/ryanmcgrath/twython.  Just from reading over each package, I really like Twython's minimalistic interface.  I will probably go with that.\n",
      "\n",
      "Sign up for Twitter developer API at https://dev.twitter.com/apps.  I named my app MovieInfoPierreDemo.  That name is so goofy, but I felt rushed!  Once it's setup I need to grab the `Consumer Key` and the `Consumer Secret`.  Its a bit confusing sorting through all the authentiction options.  The Twython documentation finally had [great advice](https://twython.readthedocs.org/en/latest/usage/starting_out.html#oauth-2-application-authentication) if all one cares about is read access to Twitter: use **Oauth2**!\n",
      "\n",
      "## Some Interesting Movie Lists\n",
      "\n",
      "Next I wanted a list of interesting movies to play with.  I found this list of titles from 2012 for\n",
      "sequels of popular movies: [Sequel Movies 2012](http://www.movieinsider.com/movies/sequel/2012).  I\n",
      "figure I'll need to manipulate some of that data by hand just to get it done quickly.  I would\n",
      "normally write some code to automate this step, but right now this is a one-time deal.\n",
      "\n",
      "## Comparing Words\n",
      "\n",
      "Once the data is all assembled into a useable form, my plan is to compare words using the [Bag-of-Words](http://en.wikipedia.org/wiki/Bag-of-words_model) approach.  This involves computing histograms of word frequencies for some ensemble of words (e.g. words collected from Tweets).  There are several ways to compare histograms with the goal of computing a similarity metric.  My favorite is the [Earth-Mover distance](http://en.wikipedia.org/wiki/Earth_mover's_distance).  It's like this: given two diffrent histograms with the same bins, think of the two distributions as two piles of dirt.  Then the EMD metric is the minimum amount of work a bulldozer have to do in order to make one pile of counts look like the other.  Last year I wrapped up this [Fast C++ EMD](http://www.seas.upenn.edu/~ofirpele/FastEMD/code/) implementation as a Python extension for a work project.\n",
      "\n",
      "In that project the distance between bins was simple: just the [Euclidean distance](http://en.wikipedia.org/wiki/Euclidean_distance) between them in bin space.   But in this task I am dealing with words as labels for each bin.  There is no physical meaning associated with which word is represented in the adjacent bin.  The words might be sorted alphabetically, or by size, or just at random.\n",
      "\n",
      "About ten years ago I implemented the [Levenshtein Distance](http://en.wikipedia.org/wiki/Levenshtein_distance) in [IDL](http://www.exelisvis.com/ProductsServices/IDL.aspx), way before I ever started using Python.  I could have translated that older IDL version over to Python, but it was actually easier to just go Googling for a Python implementation. One of the first results that came back was [py-editdist](https://code.google.com/p/py-editdist/).  That's what I love about Python: if you need a new function there's a good chance somebody already implemented something similar and made their repo publicly-available.\n",
      "\n",
      "## Random Ideas\n",
      "\n",
      "1. Maybe I can show correlationn between Twitter data and IMDb / RottenTomatoes reviews.\n",
      "1. It's not clear if EMD will be all that usefull here.\n",
      "1. compute sentiment from before/after Twitter data.  change in values after release date?\n",
      "1. potential to show temporal dependence from Twitter data, asuuming sufficient volume of tweets per day or week.\n",
      "1. curious idea for visualizing temporal sentiment.  each week compute 2D vector averaged over tweets.  Concatenate vectors into form of a chain. \n",
      "1. Anything to be done with [Mutual Information](http://en.wikipedia.org/wiki/Mutual_information)?\n",
      "1. I am going to focus on just the sentiment stuff from TextBlob.  forget about EMD / Bad-of-Words stuff for now."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Work Plan\n",
      "\n",
      "Given all that brainstorming above, let's make a plan of action!\n",
      "\n",
      "I am going to focus on acquiring data from various sources and aggregate it into a form suitable for visualization and analysis.  I don't think I'll have enough time for any exhaustive analysis.  The most I want to get done then is generating a nice visualization.\n",
      "\n",
      "1. Install necesary software packages on my laptop.\n",
      "1. Get Twitter feed connected and running: Oauth2, API key, etc.\n",
      "1. Browse through the Sequel Movies 2012 web site and make a list of interesting movies to play with.\n",
      "1. Automate looking up movie details from OMDb site.  I want information about IMDbID plus RottenTomatoes viewer feedback.  Use [Requests](http://requests.readthedocs.org/en/latest/) package.\n",
      "1. Work with Twitter API: Search for tweets about these movies over two time different periods: the months just prior to, and just after release date.\n",
      "1. Aggregate Twitter feed text for each movie (keeping before and after sets as separate collections).\n",
      "   1. what about a time period just after the sequel is announced?\n",
      "1. Apply TextBlob methods to text data pulled from RottenTomatoes and from Twitter search results.  \n",
      "1. Generate graphics (scatterplots) showing before and after sentiment(s) for original movie and sequel.\n",
      "1. Stop here for now."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Quick Example with Earth-Mover Distance\n",
      "\n",
      "*Note*: I wrote this little section before I decided to do away with EMD.  Not really relevant anymore.  Get ride of it?  Stick it in another notebook?\n",
      "\n",
      "Compute histograms of occurance of each word as basis for [Bag-of-Words](http://en.wikipedia.org/wiki/Bag-of-words_model) style model for cross-comparing multiple sets of words.  Use EMD metric as basis for quantifying similarity of two histograms.  The idea here is to compute a cost metric between all pairs of words used in this analysis.  So let's says my complete list of words is this:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# List of Interesting Movies\n",
      "I make a short list of recent movies and stored the name and reay of release in a simple [YAML](http://en.wikipedia.org/wiki/YAML) text file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fname_movies = 'movies.yml'\n",
      "\n",
      "# Run this sets of lines to view the text contents of my movie file.  Or just open up the file in your text editor.\n",
      "# with open(fname_movies) as fi:\n",
      "#      for v in fi.readlines():\n",
      "#          print(v.rstrip())\n",
      "\n",
      "info_movie_list, meta = data_io.read(fname_movies)\n",
      "\n",
      "for item in info_movie_list:\n",
      "    print('{0} ({1})'.format(item['name'], item['year']))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Hunger Games (2012)\n",
        "The Hunger Games: Catching Fire (2013)\n",
        "The Hobbit: An Unexpected Journey (2012)\n",
        "The Hobbit: The Desolation of Smaug (2013)\n",
        "Diary of a Wimpy Kid: Rodrick Rules (2011)\n",
        "Diary of a Wimpy Kid: Dog Days (2012)\n",
        "The Expendables (2010)\n",
        "The Expendables 2 (2012)\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Movie Details from IMDb and RottenTomatoes via OMDb API\n",
      "\n",
      "I found a nice web API for querrying information from both IMDb and RottenTomatoes:\n",
      "[The OMDb API](http://www.omdbapi.com/).  This site used to reside at this address http://imdbapi.com/, but not anymore.  Take a look over at the old address for an interesting writeup of the site owner's interaction with IMDb.com's lawyers.  Very clever!\n",
      "\n",
      "Anyhow, let's use this service to search for movie information by using the [Requests](http://requests.readthedocs.org/en/latest/) package to fetch data in two steps.  The first is to determine the IMDb ID number for each movie in the list.  With that information we can then pull down additional details from RottenTomatoes.  Below is an example of information about the Batman sequel:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "omdbapi_url = 'http://www.omdbapi.com'\n",
      "\n",
      "movie_name = info_movie_list[1]['name']  # index #1 should yield Hunger Games for 2013.\n",
      "movie_year = info_movie_list[1]['year']\n",
      "\n",
      "params = {'t': movie_name, 'y': movie_year, 'tomatoes': True}\n",
      "\n",
      "response = requests.get(omdbapi_url, params=params)\n",
      "info_omdb = response.json()\n",
      "\n",
      "# Use IPython's builtin display function for nicely-formatted view of the response data.\n",
      "IPython.display.display(info_omdb)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "{u'Actors': u'Jennifer Lawrence, Josh Hutcherson, Liam Hemsworth, Philip Seymour Hoffman',\n",
        " u'BoxOffice': u'$335.9M',\n",
        " u'DVD': u'N/A',\n",
        " u'Director': u'Francis Lawrence',\n",
        " u'Genre': u'Action, Adventure, Sci-Fi, Thriller',\n",
        " u'Plot': u'Katniss Everdeen and Peeta Mellark become targets of the Capitol after their victory in the 74th Hunger Games sparks a rebellion in the Districts of Panem.',\n",
        " u'Poster': u'http://ia.media-imdb.com/images/M/MV5BMTAyMjQ3OTAxMzNeQTJeQWpwZ15BbWU4MDU0NzA1MzAx._V1_SX300.jpg',\n",
        " u'Production': u'Lionsgate Films',\n",
        " u'Rated': u'PG-13',\n",
        " u'Released': u'22 Nov 2013',\n",
        " u'Response': u'True',\n",
        " u'Runtime': u'2 h 26 min',\n",
        " u'Title': u'The Hunger Games: Catching Fire',\n",
        " u'Type': u'movie',\n",
        " u'Website': u'http://www.thehungergamesexplorer.com/us/',\n",
        " u'Writer': u'Simon Beaufoy, Michael Arndt',\n",
        " u'Year': u'2013',\n",
        " u'imdbID': u'tt1951264',\n",
        " u'imdbRating': u'8.3',\n",
        " u'imdbVotes': u'69,981',\n",
        " u'tomatoConsensus': u'Smart, smoothly directed, and enriched with a deeper exploration of the franchise&#039;s thought-provoking themes, Catching Fire proves a thoroughly compelling second installment in the Hunger Games series.',\n",
        " u'tomatoFresh': u'205',\n",
        " u'tomatoImage': u'certified',\n",
        " u'tomatoMeter': u'90',\n",
        " u'tomatoRating': u'7.5',\n",
        " u'tomatoReviews': u'229',\n",
        " u'tomatoRotten': u'24',\n",
        " u'tomatoUserMeter': u'93',\n",
        " u'tomatoUserRating': u'4.4',\n",
        " u'tomatoUserReviews': u'228,552'}"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice some of the entries have some characters encoded with [ampersande encoding](http://en.wikipedia.org/wiki/Character_encodings_in_HTML).  I woud like to decode all such occurances to regular text and I found a nice implementation of just such a function over at [stackoverflow.com](http://stackoverflow.com/questions/9276848/replacement-of-ampersands-that-are-part-of-a-numeric-character-reference-by-pyth/9278304#9278304).  I am calling it from my `utilities.py` module.  This next line fixes all occurances of these encodings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Undo any ampersand encoding in the text returned from OMDbAPI.com.\n",
      "for k in info_omdb.keys():\n",
      "    info_omdb[k] = utilities.decode(info_omdb[k])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So!  There's a lot of stuff in that response data, but for now I'm going to focus on just a few pieces of data: the text strings from  both `Plot` and `tomatoConsensus`, the viewer ratings from IMDb and RottenTomatoes, plus the date the movie was released to theaters.\n",
      "\n",
      "As far as dates go, I really, **really** dislike using Python's builtin date and time tools.  The good news is there now exists a much better choice for working with dates: [Arrow](http://crsmithdev.com/arrow/).  From the web site:\n",
      "\n",
      "    Arrow is a Python library that offers a sensible, human-friendly approach to creating, manipulating, formatting\n",
      "    and converting dates, times, and timestamps.  [...] Arrow is heavily inspired by `moment.js` and `python-requests`.\n",
      "    \n",
      "Just to keep things simple here, I am going to copy out just the handful of fields I care about, and take care of date parsing at the same time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "format = 'DD MMM YYYY'\n",
      "date_released = arrow.get(info_omdb['Released'], format)\n",
      "\n",
      "# print('date_released: ', date_released.year, date_released.month, date_released.day)\n",
      "\n",
      "info_movie = {'Title': info_omdb['Title'],\n",
      "              'Plot': info_omdb['Plot'],\n",
      "              'tomatoConsensus': info_omdb['tomatoConsensus'],\n",
      "              'Released': date_released,\n",
      "              'imdbRating': float(info_omdb['imdbRating']),\n",
      "              'tomatoRating': float(info_omdb['tomatoRating'])}\n",
      "             \n",
      "IPython.display.display(info_movie)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "{u'Plot': u'Katniss Everdeen and Peeta Mellark become targets of the Capitol after their victory in the 74th Hunger Games sparks a rebellion in the Districts of Panem.',\n",
        " u'Released': <Arrow [2013-11-22T00:00:00+00:00]>,\n",
        " u'Title': u'The Hunger Games: Catching Fire',\n",
        " u'imdbRating': 8.3,\n",
        " u'tomatoConsensus': u\"Smart, smoothly directed, and enriched with a deeper exploration of the franchise's thought-provoking themes, Catching Fire proves a thoroughly compelling second installment in the Hunger Games series.\",\n",
        " u'tomatoRating': 7.5}"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Twitter Search API\n",
      "\n",
      "Following very helpful instructions from [Twython](https://twython.readthedocs.org/en/latest/usage/starting_out.html#oauth-2-application-authentication) documentation and using my own general-purpose `data_io` module for storage.  This part is **so much** easier if all you need is access to the search API, and not access to any personal info."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set this flag to True when you need to generate a new Twitter API access token.\n",
      "flag_get_new_token = False\n",
      "\n",
      "fname_twitter_api = 'twitter_api.yml'\n",
      "\n",
      "# Load Twitter API details.\n",
      "info_twitter_api, meta = data_io.read(fname_twitter_api)\n",
      "\n",
      "if not 'access_token' in info_twitter_api:\n",
      "    flag_get_new_token = True\n",
      "\n",
      "if flag_get_new_token:\n",
      "    # Use my Twitter dev API credentials to fetch a new access token.\n",
      "    twitter = twython.Twython(info_twitter_api['consumer_key'], info_twitter_api['consumer_secret'], oauth_version=2)\n",
      "    \n",
      "    print('Fetching new token...')\n",
      "    access_token = twitter.obtain_access_token()\n",
      "\n",
      "    # Store the token for later use.\n",
      "    info_twitter_api['access_token'] = access_token\n",
      "    data_io.write(fname_twitter_api, info_twitter_api)\n",
      "\n",
      "    print('New token stored: {:s}'.format(fname_twitter_api))\n",
      "\n",
      "twitter = twython.Twython(info_twitter_api['consumer_key'], access_token=info_twitter_api['access_token'])\n",
      "\n",
      "\n",
      "# This little section is the only way I know (so far) to determine if I have a valid Twitter access token\n",
      "# when using OAuth 2.\n",
      "try:\n",
      "    temp = twitter.get_application_rate_limit_status()\n",
      "except twython.TwythonAuthError:\n",
      "    msg = 'Boo hoo, you may need to regenerate your access token.'\n",
      "    raise twython.TwythonError(msg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## API Status\n",
      "Just for the fun of it here, let's print out some interesting details about my current Twitter API status.  This includes some version numbers and the current status of various rate limits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('\\nApp key:    {}'.format(twitter.app_key))\n",
      "print('OAuth version:  {}'.format(twitter.oauth_version))\n",
      "print('API version:    {}'.format(twitter.api_version))\n",
      "print('Authenticate URL: {}'.format(twitter.authenticate_url))\n",
      "\n",
      "# Rate limit stats.\n",
      "info_rate = twitter.get_application_rate_limit_status()\n",
      "\n",
      "# Application limits.\n",
      "n_limit = info_rate['resources']['application']['/application/rate_limit_status']['limit']\n",
      "n_remaining = info_rate['resources']['application']['/application/rate_limit_status']['remaining']\n",
      "t_reset = info_rate['resources']['application']['/application/rate_limit_status']['reset']\n",
      "\n",
      "delta = arrow.get(t_reset) - arrow.now() \n",
      "t_wait = delta.seconds/60\n",
      "\n",
      "print()\n",
      "print('Application limit:     {} requests'.format(n_limit))\n",
      "print('Application remaining: {} requests'.format(n_remaining))\n",
      "print('Application wait time: {:.1f} min.'.format(t_wait))\n",
      "\n",
      "# Search limits.\n",
      "n_limit = info_rate['resources']['search']['/search/tweets']['limit']\n",
      "n_remaining = info_rate['resources']['search']['/search/tweets']['remaining']\n",
      "t_reset = info_rate['resources']['search']['/search/tweets']['reset']\n",
      "\n",
      "delta = arrow.get(t_reset) - arrow.now() \n",
      "t_wait = delta.seconds/60\n",
      "\n",
      "print()\n",
      "print('Search limit:          {} requests'.format(n_limit))\n",
      "print('Search remaining:      {} requests'.format(n_remaining))\n",
      "print('Search wait time:      {:.1f} min.'.format(t_wait))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "App key:    JxUV7dXAvXigyxyWafOGUA\n",
        "OAuth version:  2\n",
        "API version:    1.1\n",
        "Authenticate URL: https://api.twitter.com/oauth/authenticate\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Application limit:     180 requests\n",
        "Application remaining: 176 requests\n",
        "Application wait time: 3.2 min.\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Search limit:          450 requests\n",
        "Search remaining:      445 requests\n",
        "Search wait time:      1.4 min.\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Twitter Search\n",
      "\n",
      "Follow search instructions using OAuth-2 at Twythow site [here](https://twython.readthedocs.org/en/latest/usage/basic_usage.html#oauth-2).  Here is a set of links to Twitter's documentation that I found most useful.  Read in this order: \n",
      "\n",
      "- [What is a Tweet](https://dev.twitter.com/docs/platform-objects/tweets)\n",
      "- [Search API: GET search/tweets](https://dev.twitter.com/docs/api/1.1/get/search/tweets)\n",
      "- [**Help with the Search API**](https://dev.twitter.com/docs/using-search)\n",
      "- [Working with Timelines](https://dev.twitter.com/docs/working-with-timelines)\n",
      " \n",
      "I also found a nice [ipython notebook](http://nbviewer.ipython.org/gist/ellisonbg/3837783/TwitterNetworkX.ipynb) online with an example using Twython, unfortunately it was for the older version 1.0 Twitter API.\n",
      "\n",
      "The page [Help with the Search API](https://dev.twitter.com/docs/using-search) has this helpful tidbit of information when you expect a large number of return tweets.  In this case it is important to pay attention to iterating through the results: \n",
      "\n",
      "> Iterating in a result set: parameters such count, until, since_id, max_id allow to control how we iterate through search results, since it could be a large set of tweets. The 'Working with Timelines' documentation is a very rich and illustrative tutorial to learn how to use these parameters to achieve the best efficiency and reliability when processing result sets.\n",
      "\n",
      "Ok, now I've read through the Twython package documenation and the source code.  The authors of his package have fully taken into account the advice above given by Twitter.  The way forward here is to use the instance method [`Twython.cursor`](https://github.com/ryanmcgrath/twython/blob/master/twython/api.py#L391).  My wrapper is now a **lot** simpler than I had earlier this afternoon! Woo!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def search_gen(q, **kwargs):\n",
      "#     \"\"\"Generator yielding individual tweets.\n",
      "#     Parameters\n",
      "#     ----------\n",
      "#     q : str, search query.\n",
      "#     pages_max : int, https://dev.twitter.com/docs/api/1/get/search  # see entry for `pages`.\n",
      "#     \"\"\"\n",
      "#     for p in range(1, pages_max+1):\n",
      "#         print(p)\n",
      "#         # Run query via Twython client.\n",
      "# #         results = twitter.search(q=q, page=str(p), **kwargs)\n",
      "#         results = twitter.search(q=q, **kwargs)\n",
      "#         1/0\n",
      "#         if not results:\n",
      "#             # All done.\n",
      "#             return        \n",
      "#         yield results['statuses']\n",
      "    \n",
      "\n",
      "def search_gen(query, since_id=None, since=None, until=None, lang='en', **kwargs):\n",
      "    \"\"\"Generator yielding individual tweets matching supplied query string.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    query : str, Twitter search query, e.g. \"python is nice\".\n",
      "    until : date string formatted as 'YYYY-MM-DD'.\n",
      "\n",
      "    \"\"\"\n",
      "    gen = twitter.cursor(twitter.search, q=query, since_id=since_id, until=until, lang=lang, **kwargs)\n",
      "    \n",
      "    for tweet in gen:\n",
      "        # Check each tweet for crap.\n",
      "        is_crappy = 'RT' in tweet['text']  # or/and ??????\n",
      "        if not is_crappy:\n",
      "            yield tweet\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "q = 'The Desolation of Smaug'\n",
      "\n",
      "num_max = 10\n",
      "gen = search_gen(q)\n",
      "\n",
      "for k, tw in enumerate(gen):\n",
      "    print(k)\n",
      "    t = tw['text'] + '\\n'\n",
      "\n",
      "    ug = re.search(\"(?P<url>https?://[^\\s]+)\", t)\n",
      "    if ug:\n",
      "        url = ug.group(\"url\")\n",
      "\n",
      "        resp = requests.get(url)        \n",
      "        soup = BeautifulSoup.BeautifulSoup(resp.content)\n",
      "\n",
      "        print('{:s} - {:s}'.format(url, soup.title.string))\n",
      "\n",
      "    print(t)\n",
      "    \n",
      "    if k > num_max:\n",
      "        break\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "http://t.co/H7GMBAvI7G - Untitled, I\u2019m watching The Hobbit: The Desolation Of..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Photo: I\u2019m watching The Hobbit: The Desolation Of Smaug \u201cThe Hobbit: The Desolation of Smaug, 4 out of 5... http://t.co/H7GMBAvI7G\n",
        "\n",
        "1\n",
        "http://t.co/BpAUpoYSco - Untitled, I just unlocked the The Hobbit: The Desolation of..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Photo: I just unlocked the The Hobbit: The Desolation of Smaug Box Office sticker on GetGlue 5025 others... http://t.co/BpAUpoYSco\n",
        "\n",
        "2\n",
        "http://t.co/7Ovo1xYvzL - Iliana @ Cin\u00e9polis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Hobbit (@ Cin\u00e9polis - @cinepolis for The Hobbit: The Desolation of Smaug w/ 7 others) http://t.co/7Ovo1xYvzL\n",
        "\n",
        "3\n",
        "THE DESOLATION OF SMAUG. DJRBKDBJDD &lt;3\n",
        "\n",
        "4\n",
        "The ending to Desolation of Smaug was such a cliff hanger... Movie was still awesome though. \ud83d\ude01\n",
        "\n",
        "5\n",
        "http://t.co/wqiDQdBRqk - \n",
        "Instagram\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Gonna watch The Hobbit: The Desolation of Smaug with my dad \u263a\ufe0f\ud83d\udc95\u2728\ud83d\ude4c http://t.co/wqiDQdBRqk\n",
        "\n",
        "6\n",
        "http://t.co/ih7iCiWW17 - Roberto @ Cin\u00e9polis VIP"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "I'm at Cin\u00e9polis VIP - @cinepolis for The Hobbit: The Desolation of Smaug (Benito Ju\u00e1rez, DF) w/ 16 others http://t.co/ih7iCiWW17\n",
        "\n",
        "7\n",
        "http://t.co/4PPpwN1xMS - Daniel @ Cinemex"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        ":D (at @Cinemex for The Hobbit: The Desolation of Smaug in 3D) http://t.co/4PPpwN1xMS\n",
        "\n",
        "8\n",
        "http://t.co/570X2FuOZe - The Hobbit: The Desolation of Smaug Entertainment Detail page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "It's another favourite day of the week! For those who have yet to catch The Hobbit: The Desolation of Smaug or... http://t.co/570X2FuOZe\n",
        "\n",
        "9\n",
        "http://t.co/5LoKW9Jigw - \n",
        "Instagram\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The Hobbit: The Desolation of Smaug - I loved this movie! I think the first one was better but this\u2026 http://t.co/5LoKW9Jigw\n",
        "\n",
        "10\n",
        "http://t.co/1itoUWncyj - Laura @ Century 16 Greenback Lane and XD"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "I'm at Century 16 Greenback Lane and XD - @cinemark for The Hobbit: The Desolation of Smaug (Sacramento, CA) http://t.co/1itoUWncyj\n",
        "\n",
        "11\n",
        "The Desolation of Smaug was so so so sos sososososo good\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "u'In The Desolation of Smaug, Gandalf said it was \"black speech\"..... Gandalf can speak ebonics?\\n'"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url='http://t.co/OhvBCTuMMD'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "resp=requests.get(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "resp.encoding"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import BeautifulSoup\n",
      "soup = BeautifulSoup.BeautifulSoup(resp.content)\n",
      "print(soup.title.string)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What about TextBlob?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}