{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from __future__ import division, print_function, unicode_literals\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import os\n",
      "\n",
      "import IPython.display\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import data_io\n",
      "import json_io\n",
      "import utilities\n",
      "\n",
      "import requests\n",
      "import BeautifulSoup\n",
      "import arrow\n",
      "import twython\n",
      "import textblob\n",
      "\n",
      "# Keyboard shortcuts: http://ipython.org/ipython-doc/stable/interactive/notebook.html#keyboard-shortcuts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Task Objective\n",
      "\n",
      "My objective for this effort is to demonstrate my data processing and analysis capabilities outside of my tradiational hyperspectral remote sensing work.  I have played in that arena for a long time and picked up a good number of modeling and analysis skills.  This present effort is meant to be a quick example of processing unfamiliar data using new tools and protocols.  This work needs to be quick, efficient, and have a clear punch line.  This notebook is where plan to explore these tools and the data they help me fetch.  I'll use another notebook for the analysis once all the data is sorted out.\n",
      "\n",
      "I made a statement a few days ago indicating that I would like to solve new types of problems.  For example, I might treat a new movie as a collection of word feature vectors pulled out of a Twitter feed.  I would then make statistical associations with other movies having known performance characteristics such as viewer retention and engagement.  The validity of the association process could be verified by testing with lablled data.  Results from such a process might be useful for someone's planning efforts.\n",
      "\n",
      "# Early Morning Thoughts\n",
      "\n",
      "## TextBlob\n",
      "\n",
      "Very very early this morning I could not sleep as I kept thinking about this little task.  I kept reviewing in my head details of the objective in the section above.  I need a way to make sense of text describing a movie.  A quick search on Github turned up this great Python package:\n",
      "[TextBlob](https://github.com/sloria/TextBlob).  The text from the website says:\n",
      "\n",
      "    A library for processing textual data. It provides a simple API for diving into common natural\n",
      "    language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction,\n",
      "    sentiment analysis, classification, translation, and more.\n",
      "\n",
      "Behind the scenes it uses the packages [NLTK](http://www.nltk.org/) and\n",
      "[patterns](http://www.clips.ua.ac.be/pages/pattern-en).  I haven't done much at all with natural\n",
      "language text processing, but this tool looks like a great place to start!  TextBlob will return two metrics describing the sentiment of a chunk of text: **Polarity** and **Subjectivity**.  Those two numbers will be a great starting point for visualizing this stuff.\n",
      "\n",
      "## Open Movie Database API: IMDb and RottenTomatoes\n",
      "\n",
      "Next, I found a nice web API for querrying information from both IMDb and RottenTomatoes:\n",
      "[The OMDb API](http://www.omdbapi.com/).  This site used to reside at this other address http://imdbapi.com/, but not anymore.  Take a look over there for an interesting writeup of the site owner's interaction with IMDb.com's lawyers.  Very clever!  Anyhow, you can use this service to very easily search for movie information pulled from IMDb and RottenTomatoes.\n",
      "\n",
      "## Twitter API\n",
      "\n",
      "I found several Python packages on Github searching for Twitter's API service, here are two that seem most well-maintained: https://github.com/geduldig/TwitterAPI and https://github.com/ryanmcgrath/twython.  Just from reading over each package, I really like Twython's minimalistic interface.  I will probably go with that.\n",
      "\n",
      "Sign up for Twitter developer API at https://dev.twitter.com/apps.  I named my app MovieInfoPierreDemo.  That name is so goofy, but I felt rushed!  Once it's setup I need to grab the `Consumer Key` and the `Consumer Secret`.  Its a bit confusing sorting through all the authentiction options.  The Twython documentation finally had [great advice](https://twython.readthedocs.org/en/latest/usage/starting_out.html#oauth-2-application-authentication) if all one cares about is read access to Twitter: use **Oauth2**!\n",
      "\n",
      "## Some Interesting Movie Lists\n",
      "\n",
      "Next I wanted a list of interesting movies to play with.  I found this list of titles from 2012 for\n",
      "sequels of popular movies: [Sequel Movies 2012](http://www.movieinsider.com/movies/sequel/2012).  I\n",
      "figure I'll need to manipulate some of that data by hand just to get it done quickly.  I would\n",
      "normally write some code to automate this step, but right now this is a one-time deal.\n",
      "\n",
      "## Comparing Words\n",
      "\n",
      "Once the data is all assembled into a useable form, my plan is to compare words using the [Bag-of-Words](http://en.wikipedia.org/wiki/Bag-of-words_model) approach.  This involves computing histograms of word frequencies for some ensemble of words (e.g. words collected from Tweets).  There are several ways to compare histograms with the goal of computing a similarity metric.  My favorite is the [Earth-Mover distance](http://en.wikipedia.org/wiki/Earth_mover's_distance).  It's like this: given two diffrent histograms with the same bins, think of the two distributions as two piles of dirt.  Then the EMD metric is the minimum amount of work a bulldozer have to do in order to make one pile of counts look like the other.  Last year I wrapped up this [Fast C++ EMD](http://www.seas.upenn.edu/~ofirpele/FastEMD/code/) implementation as a Python extension for a work project.\n",
      "\n",
      "In that project the distance between bins was simple: just the [Euclidean distance](http://en.wikipedia.org/wiki/Euclidean_distance) between them in bin space.   But in this task I am dealing with words as labels for each bin.  There is no physical meaning associated with which word is represented in the adjacent bin.  The words might be sorted alphabetically, or by size, or just at random.\n",
      "\n",
      "About ten years ago I implemented the [Levenshtein Distance](http://en.wikipedia.org/wiki/Levenshtein_distance) in [IDL](http://www.exelisvis.com/ProductsServices/IDL.aspx), way before I ever started using Python.  I could have translated that older IDL version over to Python, but it was actually easier to just go Googling for a Python implementation. One of the first results that came back was [py-editdist](https://code.google.com/p/py-editdist/).  That's what I love about Python: if you need a new function there's a good chance somebody already implemented something similar and made their repo publicly-available.\n",
      "\n",
      "## Random Ideas\n",
      "\n",
      "1. Maybe I can show correlationn between Twitter data and IMDb / RottenTomatoes reviews.\n",
      "1. It's not clear if EMD will be all that usefull here.\n",
      "1. compute sentiment from before/after Twitter data.  change in values after release date?\n",
      "1. potential to show temporal dependence from Twitter data, asuuming sufficient volume of tweets per day or week.\n",
      "1. curious idea for visualizing temporal sentiment.  each week compute 2D vector averaged over tweets.  Concatenate vectors into form of a chain. \n",
      "1. Anything to be done with [Mutual Information](http://en.wikipedia.org/wiki/Mutual_information)?\n",
      "1. I am going to focus on just the sentiment stuff from TextBlob.  forget about EMD / Bad-of-Words stuff for now."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Work Plan\n",
      "\n",
      "Given all that brainstorming above, let's make a plan of action!\n",
      "\n",
      "I am going to focus on acquiring data from various sources and aggregate it into a form suitable for visualization and analysis.  I don't think I'll have enough time for any exhaustive analysis.  The most I want to get done then is generating a nice visualization.\n",
      "\n",
      "1. Install necesary software packages on my laptop.\n",
      "1. Get Twitter feed connected and running: Oauth2, API key, etc.\n",
      "1. Browse through the Sequel Movies 2012 web site and make a list of interesting movies to play with.\n",
      "1. Automate looking up movie details from OMDb site.  I want information about IMDbID plus RottenTomatoes viewer feedback.  Use [Requests](http://requests.readthedocs.org/en/latest/) package.\n",
      "1. Work with Twitter API: Search for tweets about these movies over two time different periods: the months just prior to, and just after release date.\n",
      "1. Aggregate Twitter feed text for each movie (keeping before and after sets as separate collections).\n",
      "   1. what about a time period just after the sequel is announced?\n",
      "1. Apply TextBlob methods to text data pulled from RottenTomatoes and from Twitter search results.  \n",
      "1. Generate graphics (scatterplots) showing before and after sentiment(s) for original movie and sequel.\n",
      "1. Stop here for now."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# List of Interesting Movies\n",
      "I make a short list of recent movies and stored the name and reay of release in a simple [YAML](http://en.wikipedia.org/wiki/YAML) text file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fname_movies = 'movies.yml'\n",
      "\n",
      "# Run this sets of lines to view the text contents of my movie file.  Or just open up the file in your text editor.\n",
      "# with open(fname_movies) as fi:\n",
      "#      for v in fi.readlines():\n",
      "#          print(v.rstrip())\n",
      "\n",
      "info_movie_list, meta = data_io.read(fname_movies)\n",
      "\n",
      "for item in info_movie_list:\n",
      "    print('{0} ({1})'.format(item['name'], item['year']))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Hunger Games (2012)\n",
        "The Hunger Games: Catching Fire (2013)\n",
        "The Hobbit: An Unexpected Journey (2012)\n",
        "The Hobbit: The Desolation of Smaug (2013)\n",
        "Diary of a Wimpy Kid: Rodrick Rules (2011)\n",
        "Diary of a Wimpy Kid: Dog Days (2012)\n",
        "The Expendables (2010)\n",
        "The Expendables 2 (2012)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Movie Details from IMDb and RottenTomatoes via OMDb API\n",
      "\n",
      "I found a nice web API for querrying information from both IMDb and RottenTomatoes:\n",
      "[The OMDb API](http://www.omdbapi.com/).  This site used to reside at this address http://imdbapi.com/, but not anymore.  Take a look over at the old address for an interesting writeup of the site owner's interaction with IMDb.com's lawyers.  Very clever!\n",
      "\n",
      "Anyhow, let's use this service to search for movie information by using the [Requests](http://requests.readthedocs.org/en/latest/) package to fetch data in two steps.  The first is to determine the IMDb ID number for each movie in the list.  With that information we can then pull down additional details from RottenTomatoes.\n",
      "\n",
      "**Note**: I just found out that RottenTomatoes has their own API.  I just now signed up for an API key and have read quickly through the documentation.  It looks simple and easy-to-use.  If there's time I'll probably switch over to that instead of OMDb."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "omdbapi_url = 'http://www.omdbapi.com'\n",
      "\n",
      "movie_name = info_movie_list[1]['name']  # index #1 should yield Hunger Games for 2013.\n",
      "movie_year = info_movie_list[1]['year']\n",
      "\n",
      "params = {'t': movie_name, 'y': movie_year, 'tomatoes': True}\n",
      "\n",
      "response = requests.get(omdbapi_url, params=params)\n",
      "info_omdb = response.json()\n",
      "\n",
      "# Use IPython's builtin display function for nicely-formatted view of the response data.\n",
      "IPython.display.display(info_omdb)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "{u'Actors': u'Jennifer Lawrence, Josh Hutcherson, Liam Hemsworth, Philip Seymour Hoffman',\n",
        " u'Awards': u'N/A',\n",
        " u'BoxOffice': u'$335.9M',\n",
        " u'Country': u'N/A',\n",
        " u'DVD': u'N/A',\n",
        " u'Director': u'Francis Lawrence',\n",
        " u'Genre': u'Action, Adventure, Sci-Fi, Thriller',\n",
        " u'Language': u'N/A',\n",
        " u'Metascore': u'N/A',\n",
        " u'Plot': u'Katniss Everdeen and Peeta Mellark become targets of the Capitol after their victory in the 74th Hunger Games sparks a rebellion in the Districts of Panem.',\n",
        " u'Poster': u'http://ia.media-imdb.com/images/M/MV5BMTAyMjQ3OTAxMzNeQTJeQWpwZ15BbWU4MDU0NzA1MzAx._V1_SX300.jpg',\n",
        " u'Production': u'Lionsgate Films',\n",
        " u'Rated': u'PG-13',\n",
        " u'Released': u'22 Nov 2013',\n",
        " u'Response': u'True',\n",
        " u'Runtime': u'2 h 26 min',\n",
        " u'Title': u'The Hunger Games: Catching Fire',\n",
        " u'Type': u'movie',\n",
        " u'Website': u'http://www.thehungergamesexplorer.com/us/',\n",
        " u'Writer': u'Simon Beaufoy, Michael Arndt',\n",
        " u'Year': u'2013',\n",
        " u'imdbID': u'tt1951264',\n",
        " u'imdbRating': u'8.3',\n",
        " u'imdbVotes': u'69,981',\n",
        " u'tomatoConsensus': u'Smart, smoothly directed, and enriched with a deeper exploration of the franchise&#039;s thought-provoking themes, Catching Fire proves a thoroughly compelling second installment in the Hunger Games series.',\n",
        " u'tomatoFresh': u'205',\n",
        " u'tomatoImage': u'certified',\n",
        " u'tomatoMeter': u'90',\n",
        " u'tomatoRating': u'7.5',\n",
        " u'tomatoReviews': u'229',\n",
        " u'tomatoRotten': u'24',\n",
        " u'tomatoUserMeter': u'93',\n",
        " u'tomatoUserRating': u'4.4',\n",
        " u'tomatoUserReviews': u'228,552'}"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice some of the entries have some characters encoded with [ampersande encoding](http://en.wikipedia.org/wiki/Character_encodings_in_HTML).  I woud like to decode all such occurances to regular text and I found a nice implementation of just such a function over at [stackoverflow.com](http://stackoverflow.com/questions/9276848/replacement-of-ampersands-that-are-part-of-a-numeric-character-reference-by-pyth/9278304#9278304).  I am calling it from my `utilities.py` module.  This next line fixes all occurances of these encodings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Undo any ampersand encoding in the text returned from OMDbAPI.com.\n",
      "for k in info_omdb.keys():\n",
      "    info_omdb[k] = utilities.decode(info_omdb[k])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So!  There's a lot of stuff in that response data, but for now I'm going to focus on just a few pieces of data: the text strings corresponding to the keys `Plot` and `tomatoConsensus`, the viewer ratings from IMDb and RottenTomatoes, plus the date the movie was released to theaters.  Just to keep things simple here, I am going to copy out only the handful of fields I care about, and take care of date parsing at the same time.\n",
      "\n",
      "As far as dates go, I really, **really** dislike using Python's builtin date and time tools.  The good news is there now exists a much better choice for working with dates: [Arrow](http://crsmithdev.com/arrow/).  From the web site:\n",
      "\n",
      "> Arrow is a Python library that offers a sensible, human-friendly approach to creating, manipulating, formatting and converting dates, times, and timestamps.  [...] Arrow is heavily inspired by `moment.js` and `python-requests`.\n",
      "\n",
      "See here http://crsmithdev.com/arrow/#format and here http://crsmithdev.com/arrow/#tokens for date/time format details."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "format = 'DD MMM YYYY'\n",
      "date_released = arrow.get(info_omdb['Released'], format)\n",
      "\n",
      "# print('date_released: ', date_released.year, date_released.month, date_released.day)\n",
      "\n",
      "info_movie = {'Title': info_omdb['Title'],\n",
      "              'Plot': info_omdb['Plot'],\n",
      "              'tomatoConsensus': info_omdb['tomatoConsensus'],\n",
      "              'Released': date_released,\n",
      "              'imdbRating': float(info_omdb['imdbRating']),\n",
      "              'tomatoRating': float(info_omdb['tomatoRating'])}\n",
      "             \n",
      "IPython.display.display(info_movie)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "{u'Plot': u'Katniss Everdeen and Peeta Mellark become targets of the Capitol after their victory in the 74th Hunger Games sparks a rebellion in the Districts of Panem.',\n",
        " u'Released': <Arrow [2013-11-22T00:00:00+00:00]>,\n",
        " u'Title': u'The Hunger Games: Catching Fire',\n",
        " u'imdbRating': 8.3,\n",
        " u'tomatoConsensus': u\"Smart, smoothly directed, and enriched with a deeper exploration of the franchise's thought-provoking themes, Catching Fire proves a thoroughly compelling second installment in the Hunger Games series.\",\n",
        " u'tomatoRating': 7.5}"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Rotten Tomatoes API\n",
      "\n",
      " - URL: http://developer.rottentomatoes.com/docs\n",
      "\n",
      " - API Key: mfy52ff3xbgcdwxqr9fwvjw9\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uri_base = 'http://api.rottentomatoes.com/api/public/v1.0'\n",
      "uri_home = 'http://api.rottentomatoes.com/api/public/v1.0.json?apikey=mfy52ff3xbgcdwxqr9fwvjw9'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Twitter's Search API\n",
      "\n",
      "Following very helpful instructions from [Twython](https://twython.readthedocs.org/en/latest/usage/starting_out.html#oauth-2-application-authentication) documentation and using my own general-purpose `data_io` module for storage.  This part is **so much** easier if all you need is access to the search API, and not access to any personal info."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set this flag to True when you need to generate a new Twitter API access token.\n",
      "flag_get_new_token = False\n",
      "\n",
      "fname_twitter_api = 'twitter_api.yml'\n",
      "\n",
      "# Load Twitter API details.\n",
      "info_twitter_api, meta = data_io.read(fname_twitter_api)\n",
      "\n",
      "if not 'access_token' in info_twitter_api:\n",
      "    flag_get_new_token = True\n",
      "\n",
      "if flag_get_new_token:\n",
      "    # Use my Twitter dev API credentials to fetch a new access token.\n",
      "    twitter = twython.Twython(info_twitter_api['consumer_key'], info_twitter_api['consumer_secret'], oauth_version=2)\n",
      "    \n",
      "    print('Fetching new token...')\n",
      "    access_token = twitter.obtain_access_token()\n",
      "\n",
      "    # Store the token for later use.\n",
      "    info_twitter_api['access_token'] = access_token\n",
      "    data_io.write(fname_twitter_api, info_twitter_api)\n",
      "\n",
      "    print('New token stored: {:s}'.format(fname_twitter_api))\n",
      "else:\n",
      "    twitter = twython.Twython(info_twitter_api['consumer_key'], access_token=info_twitter_api['access_token'])\n",
      "\n",
      "# This little try/except section is the only way I know (so far) to determine if I have a valid Twitter access token\n",
      "# when using OAuth 2.\n",
      "try:\n",
      "    temp = twitter.get_application_rate_limit_status()\n",
      "except twython.TwythonAuthError:\n",
      "    msg = 'Boo hoo, you may need to regenerate your access token.'\n",
      "    raise twython.TwythonError(msg)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## API Current Status Query\n",
      "Just for the fun of it here, let's print out some interesting tidbits about the current status of my Twitter API key.  This includes some version numbers and the current status of various rate limits.  If you are going to be calling the API frequently you might run against these rate limits.  Watch out!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('\\nApp key:    {}'.format(twitter.app_key))\n",
      "print('OAuth version:  {}'.format(twitter.oauth_version))\n",
      "print('API version:    {}'.format(twitter.api_version))\n",
      "print('Authenticate URL: {}'.format(twitter.authenticate_url))\n",
      "\n",
      "# Rate limit stats.\n",
      "info_rate = twitter.get_application_rate_limit_status()\n",
      "\n",
      "# Application limits.\n",
      "n_limit = info_rate['resources']['application']['/application/rate_limit_status']['limit']\n",
      "n_remaining = info_rate['resources']['application']['/application/rate_limit_status']['remaining']\n",
      "t_reset = info_rate['resources']['application']['/application/rate_limit_status']['reset']\n",
      "\n",
      "delta = arrow.get(t_reset) - arrow.now() \n",
      "t_wait = delta.seconds/60\n",
      "\n",
      "print()\n",
      "print('Application limit:     {} requests'.format(n_limit))\n",
      "print('Application remaining: {} requests'.format(n_remaining))\n",
      "print('Application wait time: {:.1f} min.'.format(t_wait))\n",
      "\n",
      "# Search limits.\n",
      "n_limit = info_rate['resources']['search']['/search/tweets']['limit']\n",
      "n_remaining = info_rate['resources']['search']['/search/tweets']['remaining']\n",
      "t_reset = info_rate['resources']['search']['/search/tweets']['reset']\n",
      "\n",
      "delta = arrow.get(t_reset) - arrow.now() \n",
      "t_wait = delta.seconds/60\n",
      "\n",
      "print()\n",
      "print('Search limit:          {} requests'.format(n_limit))\n",
      "print('Search remaining:      {} requests'.format(n_remaining))\n",
      "print('Search wait time:      {:.1f} min.'.format(t_wait))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "App key:    JxUV7dXAvXigyxyWafOGUA\n",
        "OAuth version:  2\n",
        "API version:    1.1\n",
        "Authenticate URL: https://api.twitter.com/oauth/authenticate\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Application limit:     180 requests\n",
        "Application remaining: 178 requests\n",
        "Application wait time: 14.9 min.\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Search limit:          450 requests\n",
        "Search remaining:      440 requests\n",
        "Search wait time:      11.5 min.\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Twitter Search\n",
      "\n",
      "Follow search instructions using OAuth-2 at Twythow site [here](https://twython.readthedocs.org/en/latest/usage/basic_usage.html#oauth-2).  The following set of links to Twitter's documentation that I found most useful:\n",
      "\n",
      "- [What is a Tweet](https://dev.twitter.com/docs/platform-objects/tweets)\n",
      "- [Search API: GET search/tweets](https://dev.twitter.com/docs/api/1.1/get/search/tweets)\n",
      "- [**Help with the Search API**](https://dev.twitter.com/docs/using-search)\n",
      "- [Working with Timelines](https://dev.twitter.com/docs/working-with-timelines)\n",
      " \n",
      "I also found a nice [ipython notebook](http://nbviewer.ipython.org/gist/ellisonbg/3837783/TwitterNetworkX.ipynb) online showing an example using Twython, unfortunately it was for the older version 1.0 Twitter API.  The implementation details have changed with Twitter's API version 1.1.\n",
      "\n",
      "The page [Help with the Search API](https://dev.twitter.com/docs/using-search) has this helpful tidbit of information when you expect a large number of return tweets.  In this case it is important to pay attention to iterating through the results: \n",
      "\n",
      "> Iterating in a result set: parameters such count, until, since_id, max_id allow to control how we iterate through search results, since it could be a large set of tweets. The 'Working with Timelines' documentation is a very rich and illustrative tutorial to learn how to use these parameters to achieve the best efficiency and reliability when processing result sets.\n",
      "\n",
      "Ok, now I've read through the Twython package documenation and the source code.  The authors of his package have fully taken into account the advice above given by Twitter.  The way forward here is to use the instance method [`Twython.cursor`](https://github.com/ryanmcgrath/twython/blob/master/twython/api.py#L391).  My wrapper is now a **lot** simpler than what I had earlier this afternoon! Woo!\n",
      "\n",
      "## Contents of a Tweet\n",
      "\n",
      "Here is an example of the JSON contents of a tweet returned through the Twitter API.\n",
      "\n",
      "```json\n",
      "{'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Sat Dec 28 17:26:58 +0000 2013',\n",
      " 'entities': {'hashtags': [],\n",
      "              'symbols': [], \n",
      "              'urls': [],\n",
      "              'user_mentions': [{'id': 848116975,\n",
      "                                  'id_str': '848116975',\n",
      "                                  'indices': [42, 58],\n",
      "                                  'name': 'ZZZZZZZ',\n",
      "                                  'screen_name': 'QQQQQQQ'},\n",
      "                                 {'id': 2202651295,\n",
      "                                  'id_str': '2202651295',\n",
      "                                  'indices': [59, 73],\n",
      "                                  'name': 'XXXXX',\n",
      "                                  'screen_name': 'YYYYY'}]},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'geo': None,\n",
      " 'id': 416983627935670272,\n",
      " 'id_str': '416983627935670272',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'lang': 'en',\n",
      " 'metadata': {'iso_language_code': u'en', u'result_type': u'recent'},\n",
      " 'place': None,\n",
      " 'retweet_count': 0,\n",
      " 'retweeted': False,\n",
      " 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
      " 'text': 'Going to see the Desolation of Smaug with @AllenWellingto1 @hannahkovacs3',\n",
      " 'truncated': False,\n",
      " 'user': { XXXXX }}\n",
      "```\n",
      "  \n",
      "## Some helper code.\n",
      "\n",
      "What follows in the next cell are a few helper classes and functions to make future work easier and more fun."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Tweet(object):\n",
      "    def __init__(self, json):\n",
      "        self.json = json\n",
      "        \n",
      "    @property\n",
      "    def has_url(self):\n",
      "        return self.json['entities']['urls']\n",
      "    \n",
      "    @property\n",
      "    def url_title(self):\n",
      "        \"\"\"Return title of first URL page, if URL exists.\n",
      "        \"\"\"\n",
      "        \n",
      "        if self.has_url:\n",
      "            # Grab the first URL.\n",
      "            url =  self.json['entities']['urls'][0]['expanded_url']\n",
      "            \n",
      "            resp = requests.get(url)        \n",
      "            soup = BeautifulSoup.BeautifulSoup(resp.content)\n",
      "            results = soup.title.string\n",
      "        else:\n",
      "            results = None\n",
      "            \n",
      "        return results\n",
      "        \n",
      "    @property\n",
      "    def is_retweet(self):\n",
      "        \"\"\"Indicate if this tweet is a retweet.\n",
      "        https://dev.twitter.com/docs/platform-objects/tweets\n",
      "        \"\"\"\n",
      "        return 'retweeted_status' in self.json\n",
      "        \n",
      "    @property\n",
      "    def text(self):\n",
      "        results = self.json['text']\n",
      "\n",
      "        # Check to see if there any URLs embedded in text.\n",
      "        if self.json['entities']['urls']:\n",
      "            # Grab the first URL, crop ll URLs from text.\n",
      "            ixs = self.json['entities']['urls'][0]['indices']            \n",
      "            results = results[:ixs[0]]\n",
      "            \n",
      "        return results\n",
      "    \n",
      "    @property\n",
      "    def id(self):\n",
      "        \"\"\"Twitter tweet ID.\n",
      "        \"\"\"\n",
      "        return  int(self.json['id_str'])\n",
      "    \n",
      "    @property\n",
      "    def timestamp(self):\n",
      "        \"\"\"Time when Tweet was created.\n",
      "\n",
      "        # e.g. Sat Dec 28 16:56:41 +0000 2013'\n",
      "        \"\"\"\n",
      "\n",
      "        format = 'ddd MMM DD HH:mm:ss Z YYYY'\n",
      "        stamp = arrow.get(self.json['created_at'], format)\n",
      "    \n",
      "        return stamp\n",
      "\n",
      "\n",
      "    def to_file(self, fname):\n",
      "        \"\"\"Serialize this Tweet to a JSON file.\n",
      "        \"\"\"\n",
      "        b, e = os.path.splitext(fname)\n",
      "        fname = b + '.json'\n",
      "        \n",
      "        json_io.write(fname, self.json)\n",
      "\n",
      "    \n",
      "    @staticmethod\n",
      "    def from_file(fname):\n",
      "        \"\"\"Instanciate a Tweet object from previously-serialized Tweet.\n",
      "        \"\"\"\n",
      "        b, e = os.path.splitext(fname)\n",
      "        fname = b + '.json'\n",
      "\n",
      "        json = json_io.read(fname)\n",
      "        \n",
      "        tw = Tweet(json)\n",
      "        return tw\n",
      "            \n",
      "#######################################################################\n",
      "\n",
      "            \n",
      "def search_gen(query, since_id=None, since=None, until=None, lang='en', **kwargs):\n",
      "    \"\"\"Generator yielding individual tweets matching supplied query string.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    query : str, Twitter search query, e.g. \"python is nice\".\n",
      "    until : date string formatted as 'YYYY-MM-DD'.\n",
      "\n",
      "    \"\"\"\n",
      "    gen = twitter.cursor(twitter.search, q=query, since_id=since_id, until=until, lang=lang, **kwargs)\n",
      "    \n",
      "    for json in gen:\n",
      "        tw = Tweet(json)\n",
      "\n",
      "        # Check each tweet for crap.\n",
      "        is_crappy = tw.is_retweet or tw.has_url\n",
      "\n",
      "        if not is_crappy:\n",
      "            yield tw\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try running a quick query for recent tweets about the current Hobbit movie.  Notice below that I am also searching for any URLs in the text.  I use a combination of `Requests` and `BeautifulSoup` to fetch the title of whatever page is at the other end of that URL. In addition to the **text** of the actual tweet, I also want the **time** (UTC), **date**, and **geographic location**.\n",
      "\n",
      "Use `id_str` instead of `id`.  See this discussion for details https://groups.google.com/forum/#!topic/twitter-development-talk/ahbvo3VTIYI."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Practice search on a topic and extracting information from returned tweets. \n",
      "q = 'Star Wars'\n",
      "\n",
      "num_max = 15\n",
      "gen = search_gen(q)\n",
      "\n",
      "for k, tw in enumerate(gen):\n",
      "    print('\\ntweet: {:d}'.format(k))\n",
      "    print('id: {:d}'.format(tw.id))\n",
      "    print(tw.text)\n",
      "    \n",
      "    if k > num_max:\n",
      "        break\n",
      "        \n",
      "    # Save tweet to JSON file.\n",
      "#     name = 'tweet_{:s}_'.format(tw['id_str'])\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tweet: 0\n",
        "id: 417003839670484993\n",
        "An hour ago i was watching Star Wars with no hope and now I am flying off the walls with a dangerous amount\n",
        "\n",
        "tweet: 1\n",
        "id: 417003834204893184\n",
        "Where do prople buy Star Wars stuff? In a Star Wars store BY: Harry Styles\n",
        "\n",
        "tweet: 2\n",
        "id: 417003830514298880\n",
        "This year's parody is 'Shakespeare does Star Wars' http://t.co/WO82I1TBxP\n",
        "\n",
        "tweet: 3\n",
        "id: 417003815435767808\n",
        "'bout to watch Star Wars, @Lunsfuhd would be proud!\n",
        "\n",
        "tweet: 4\n",
        "id: 417003803473620992\n",
        "Star Wars! \u263a\ufe0f\ud83d\udc4c\u2728\n",
        "\n",
        "tweet: 5\n",
        "id: 417003783793942528\n",
        "@lheidensohn97 err they aren't cool enough to be star wars ://\n",
        "\n",
        "tweet: 6\n",
        "id: 417003753255235584\n",
        "Watching Star Wars with my son for the 1st time...\n",
        "\n",
        "#GLUED\n",
        "\n",
        "tweet: 7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "id: 417003839670484993\n",
        "An hour ago i was watching Star Wars with no hope and now I am flying off the walls with a dangerous amount\n",
        "\n",
        "tweet: 8\n",
        "id: 417003834204893184\n",
        "Where do prople buy Star Wars stuff? In a Star Wars store BY: Harry Styles\n",
        "\n",
        "tweet: 9\n",
        "id: 417003830514298880\n",
        "This year's parody is 'Shakespeare does Star Wars' http://t.co/WO82I1TBxP\n",
        "\n",
        "tweet: 10\n",
        "id: 417003815435767808\n",
        "'bout to watch Star Wars, @Lunsfuhd would be proud!\n",
        "\n",
        "tweet: 11\n",
        "id: 417003803473620992\n",
        "Star Wars! \u263a\ufe0f\ud83d\udc4c\u2728\n",
        "\n",
        "tweet: 12\n",
        "id: 417003783793942528\n",
        "@lheidensohn97 err they aren't cool enough to be star wars ://\n",
        "\n",
        "tweet: 13\n",
        "id: 417003753255235584\n",
        "Watching Star Wars with my son for the 1st time...\n",
        "\n",
        "#GLUED\n",
        "\n",
        "tweet: 14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "id: 417003839670484993\n",
        "An hour ago i was watching Star Wars with no hope and now I am flying off the walls with a dangerous amount\n",
        "\n",
        "tweet: 15\n",
        "id: 417003834204893184\n",
        "Where do prople buy Star Wars stuff? In a Star Wars store BY: Harry Styles\n",
        "\n",
        "tweet: 16\n",
        "id: 417003830514298880\n",
        "This year's parody is 'Shakespeare does Star Wars' http://t.co/WO82I1TBxP\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<__main__.Tweet at 0x5a36950>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t=Tweet(tw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t.is_retweet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "argument of type 'Tweet' is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-13-dad0e516f765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_retweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-9-2e2f18896150>\u001b[0m in \u001b[0;36mis_retweet\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \"\"\"\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'retweeted_status'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: argument of type 'Tweet' is not iterable"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What about TextBlob?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}